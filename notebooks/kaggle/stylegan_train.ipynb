{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1) Mount drive, unzip data, clone repo, install packages\n\n## 1.1) Define paths\nGoogle Drive root: `/kaggle/working/GoogleDrive`\nDataset paths are relative to mount point (`/kaggle/input` & `/kaggle/working`).","metadata":{}},{"cell_type":"code","source":"import os\n\n# Create root directory if not exists\ndrive_root = '/kaggle/working/GoogleDrive'\n!mkdir -p \"$drive_root\"\n\n# Define Google Drive related paths\n!mkdir -p \"$drive_root\"\n!mkdir -p \"$drive_root/Models\"\n!mkdir -p \"$drive_root/Datasets\"\n!mkdir -p \"$drive_root/GitHub Keys\"\n\n# Define asset paths\ngit_keys_root = '/kaggle/input/git-keys/github-keys'\nassert os.path.exists(git_keys_root), f'git_keys_root={git_keys_root}: NOT FOUND'\nclient_secrets_path = '/kaggle/input/git-keys/client_secrets.json'\nassert os.path.exists(client_secrets_path), f'client_secrets_path={client_secrets_path}: NOT FOUND'","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:16:37.907157Z","iopub.execute_input":"2022-03-07T19:16:37.907772Z","iopub.status.idle":"2022-03-07T19:16:41.198311Z","shell.execute_reply.started":"2022-03-07T19:16:37.907681Z","shell.execute_reply":"2022-03-07T19:16:41.197185Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1.2) Link inputs available via Kaggle Datasets to corresponding GoogleDrive paths","metadata":{}},{"cell_type":"code","source":"# Copy the Fonts dir inside local Google Drive root\n!cp -rf /kaggle/input/mplfonts/Fonts \"$drive_root\"\n\n# Link the Inceptionv3 model Checkpoint inside local Google Drive root\n!mkdir -p \"$drive_root\"/Models\n!cp -rf \"/kaggle/input/inception-model/model_name=inceptionv3\" \"$drive_root\"/Models\n!mv \"$drive_root\"/Models/model_name=inceptionv3/Checkpoints/1a9a5a14.pth.bak \"$drive_root\"/Models/model_name=inceptionv3/Checkpoints/1a9a5a14.pth\n\n# Link the VGG model Checkpoint inside local Google Drive root\n!mkdir -p \"$drive_root\"/Models\n!cp -r \"/kaggle/input/inception-model/model_name=vgg16\" \"$drive_root\"/Models\n!cp -r \"/kaggle/input/inception-model/model_name=vgg16sliced\" \"$drive_root\"/Models\n!cp -r \"/kaggle/input/inception-model/model_name=vgg16perceptual\" \"$drive_root\"/Models\n\n# FIX: We need client_secrets.json to be writable, so copy to /kaggle/working\nif not os.path.exists(os.path.join(drive_root, 'client_secrets.json')):\n    !cp -f \"$client_secrets_path\" \"$drive_root\"\nclient_secrets_path = f'{drive_root}/client_secrets.json'\n\n# Link Dataset\n!mkdir -p \"$drive_root\"/Datasets/DeepFashion/Fashion\\ Synthesis\\ Benchmark\n!ln -s /kaggle/input/deepfashion-fisb/Img.h5 \"$drive_root\"/Datasets/DeepFashion/Fashion\\ Synthesis\\ Benchmark\n!ln -s /kaggle/input/deepfashion-fisb/crops.json \"$drive_root\"/Datasets/DeepFashion/Fashion\\ Synthesis\\ Benchmark\n!ln -s /kaggle/input/deepfashion-fisb/backgrounds.json \"$drive_root\"/Datasets/DeepFashion/Fashion\\ Synthesis\\ Benchmark\n!ls \"$drive_root\"/Datasets/DeepFashion/Fashion\\ Synthesis\\ Benchmark","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:16:41.202144Z","iopub.execute_input":"2022-03-07T19:16:41.202437Z","iopub.status.idle":"2022-03-07T19:17:03.629505Z","shell.execute_reply.started":"2022-03-07T19:16:41.202405Z","shell.execute_reply":"2022-03-07T19:17:03.628485Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1.3) Clone GitHub repo\nClone achariso/gans-thesis repo into /kaggle/working/code using git clone. For a similar procedure in Colab,\nsee: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f","metadata":{}},{"cell_type":"code","source":"# Clean failed attempts\n!rm -rf /root/.ssh\n!rm -rf /kaggle/working/code\n!mkdir -p /kaggle/working/code\n\nrepo_root = '/kaggle/working/code/gans-thesis'\nif not os.path.exists(repo_root):\n    # Check that ssh keys exist\n    id_rsa_abs_drive = f'{git_keys_root}/id_rsa'\n    id_rsa_pub_abs_drive = f'{id_rsa_abs_drive}.pub'\n    assert os.path.exists(id_rsa_abs_drive)\n    assert os.path.exists(id_rsa_pub_abs_drive)\n    # On first run: Add ssh key in repo\n    if not os.path.exists('/root/.ssh'):\n        # Transfer config file\n        ssh_config_abs_drive = f'{git_keys_root}/config'\n        assert os.path.exists(ssh_config_abs_drive)\n        !mkdir -p ~/.ssh\n        !cp -f \"$ssh_config_abs_drive\" ~/.ssh/\n        # Add github.com to known hosts\n        !ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n        # Test ssh connection\n        # !ssh -T git@github.com\n\n    # Remove any previous attempts\n    !rm -rf \"$repo_root\"\n    !mkdir -p \"$repo_root\"\n    # Clone repo\n    !git clone git@github.com:achariso/gans-thesis.git \"$repo_root\"\n    src_root = f'{repo_root}/src'\n    !rm -rf \"$repo_root\"/report","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:06.422545Z","iopub.execute_input":"2022-03-07T19:18:06.422919Z","iopub.status.idle":"2022-03-07T19:18:14.126588Z","shell.execute_reply.started":"2022-03-07T19:18:06.422872Z","shell.execute_reply":"2022-03-07T19:18:14.125246Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 1.4) Install pip packages\nAll required files are stored in a requirements.txt files at the repository's root.\nUse `pip install -r requirements.txt` from inside the dir to install required packages.","metadata":{}},{"cell_type":"code","source":"%cd \"$repo_root\"\n!pip install -r requirements.txt\n\n# import os\n# os.kill(os.getpid(), 9)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:14.130151Z","iopub.execute_input":"2022-03-07T19:18:14.130444Z","iopub.status.idle":"2022-03-07T19:18:20.584175Z","shell.execute_reply.started":"2022-03-07T19:18:14.130411Z","shell.execute_reply":"2022-03-07T19:18:20.583165Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nassert torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:20.586258Z","iopub.execute_input":"2022-03-07T19:18:20.586629Z","iopub.status.idle":"2022-03-07T19:18:21.057254Z","shell.execute_reply.started":"2022-03-07T19:18:20.586589Z","shell.execute_reply":"2022-03-07T19:18:21.056196Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 1.5) Add code/, */src/ to path\nThis is necessary in order to be able to run the modules.","metadata":{}},{"cell_type":"code","source":"content_root_abs = f'{repo_root}'\nsrc_root_abs = f'{repo_root}/src'\n%env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:21.059349Z","iopub.execute_input":"2022-03-07T19:18:21.059859Z","iopub.status.idle":"2022-03-07T19:18:21.066149Z","shell.execute_reply.started":"2022-03-07T19:18:21.059820Z","shell.execute_reply":"2022-03-07T19:18:21.065242Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 2) Train StyleGAN model on DeepFashion's Fashion Image Synthesis Benchmark dataset\nIn this section we run the actual training loop for StyleGAN network. StyleGAN consists of a stylized generator and a fairly naive discriminator architecture. Both however are progressively grown, starting from a resolution of 4x4 up to the final resolution fo 128x128.","metadata":{}},{"cell_type":"markdown","source":"### Environment Setup\n\nSetup/preparation before starting StyleGAN training loop.","metadata":{}},{"cell_type":"code","source":"# !cat /kaggle/working/GoogleDrive/client_secrets.json\n# !cat /kaggle/input/git-keys/client_secrets.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chkpt_step = 'latest'       # supported: 'latest', <int>, None\nlog_level = 'debug'         # supported: 'debug', 'info', 'warning', 'error', 'critical', 'fatal'\ndevice = 'cuda'             # supported: 'cpu', 'cuda', 'cuda:<GPU_INDEX>'\ngdrive_which = 'personal'   # supported: 'personal', 'auth'\n\n# Running with -i enables us to get variables defined inside the script (the script runs inline)\n%run -i src/train_setup.py --log_level $log_level --chkpt_step $chkpt_step --seed 42 --device $device --gdrive_which $gdrive_which -use_refresh_token ","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:21.067524Z","iopub.execute_input":"2022-03-07T19:18:21.067900Z","iopub.status.idle":"2022-03-07T19:18:23.019777Z","shell.execute_reply.started":"2022-03-07T19:18:21.067852Z","shell.execute_reply":"2022-03-07T19:18:23.018607Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### StyleGAN Training\n\nEventually, run the code!","metadata":{}},{"cell_type":"code","source":"%cd 'src/'\n\nimport torch\nfrom IPython.core.display import display\nfrom PIL import Image\nfrom torch import Tensor\nfrom torch.nn import DataParallel\n# noinspection PyProtectedMember\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\n\nfrom datasets.deep_fashion import FISBDataset, FISBDataloader\nfrom modules.stylegan import StyleGan\nfrom tqdm import tqdm\nfrom utils.dep_free import get_tqdm\nfrom utils.ifaces import FilesystemDataset\nfrom utils.metrics import GanEvaluator\n\n###################################\n###  Hyper-parameters settings  ###\n###################################\n#   - training\nn_epochs = 500\nbatch_size = 32 if not run_locally else 4\ntrain_test_splits = [90, 10]  # for a 90% training - 10% evaluation set split\n#   - evaluation\nmetrics_n_samples = 1000 if not run_locally else 2\nmetrics_batch_size = 32 if not run_locally else 1\nf1_k = 3 if not run_locally else 1\n#   - visualizations / checkpoints steps\ndisplay_steps = {4: 40, 8: 80, 16: 120, 32: 160, 64: 200, 128: 300}\ncheckpoint_steps = {k:3*v for k,v in display_steps.items()}\nmetrics_steps = {k:3*v for k,v in checkpoint_steps.items()}  # evaluate model every 3 checkpoints\n#   - dataset\ntarget_shape = 128\ntarget_channels = 3\n#   - StyleGAN configuration\nz_dim = 512\nuse_half_precision = False\nstgan_config_id = f'karrasA_z{z_dim}_16gb' if not use_half_precision \\\n    else f'karrasA_z{z_dim}_half_16gb'\n\n###################################\n###   Dataset Initialization    ###\n###################################\n#   - image transforms:\n#     If target_shape is different from load one, resize & crop. If target_shape is different from load shape,\n#     convert to grayscale.\n#     Update: Now done automatically if you set target_channels, target_shape when instantiating the dataloader.\ngen_transforms = FISBDataset.get_image_transforms(target_shape=target_shape, target_channels=target_channels)\n#   - the dataloader used to access the training dataset of cross-scale/pose image pairs at every epoch\n#     > len(dataloader) = <number of batches>\n#     > len(dataloader.dataset) = <number of total dataset items>\ndataloader = FISBDataloader(dataset_fs_folder_or_root=datasets_groot, batch_size=batch_size, log_level=log_level,\n                            image_transforms=gen_transforms, splits=train_test_splits, pin_memory=not run_locally,\n                            load_in_memory=False, min_color='#f0f0f0')\ndataset = dataloader.dataset  # save training dataset as `dataset`\n#   - ensure dataset is fetched locally and unzipped\nif isinstance(dataset, FilesystemDataset):\n    dataset.fetch_and_unzip(in_parallel=False, show_progress=True)\nelif hasattr(dataset, 'dataset') and isinstance(dataset.dataset, FilesystemDataset):\n    dataset.dataset.fetch_and_unzip(in_parallel=False, show_progress=True)\nelse:\n    raise TypeError('dataset must implement utils.ifaces.FilesystemDataset in order to be auto-downloaded and unzipped')\n#   - apply rudimentary tests\nassert issubclass(dataloader.__class__, DataLoader)\nassert len(dataloader) == len(dataset) // batch_size + (1 if len(dataset) % batch_size else 0)\n_real = next(iter(dataloader))\nassert tuple(_real.shape) == (batch_size, target_channels, target_shape, target_shape)\n\n###################################\n###    Models Initialization    ###\n###################################\n#   - initialize evaluator instance (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\nevaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataset, z_dim=z_dim, device=exec_device,\n                         n_samples=metrics_n_samples, batch_size=metrics_batch_size, f1_k=f1_k)\n#   - initialize model\nchkpt_step = args.chkpt_step\ntry:\n    if chkpt_step == 'latest':\n        stgan_chkpt_step = chkpt_step\n    elif isinstance(chkpt_step, str) and chkpt_step.isdigit():\n        stgan_chkpt_step = int(chkpt_step)\n    else:\n        stgan_chkpt_step = None\nexcept NameError:\n    stgan_chkpt_step = None\nstgan = StyleGan(model_fs_folder_or_root=models_groot, config_id=stgan_config_id, dataset_len=len(dataset),\n                 chkpt_epoch=stgan_chkpt_step, evaluator=evaluator, device=exec_device, log_level=log_level)\nstgan.logger.debug(f'Using device: {str(exec_device)}')\nstgan.logger.debug(f'Model initialized. Number of params = {stgan.nparams_hr}')\n# FIX: Warmup counters before first batch\nif stgan.step is None:\n    stgan.gforward(batch_size=batch_size)\n    stgan.logger.debug(f'Model warmed-up (internal counters).')\n# FIX: Dataloader batch_size need update\nif stgan.current_batch_size is not None and stgan.current_batch_size != batch_size:\n    stgan.logger.debug(f'Updating Dataloader batch_size (from {batch_size} --> {stgan.current_batch_size}).')\n    batch_size = stgan.current_batch_size\n    dataloader = dataloader.update_batch_size(batch_size=batch_size)\n#   - setup multi-GPU training\nif torch.cuda.device_count() > 1:\n    stgan.gen = DataParallel(stgan.gen)\n    stgan.info(f'Using {torch.cuda.device_count()} GPUs for StyleGAN Generator (via torch.nn.DataParallel)')\n#   - load dataloader state (from model checkpoint)\nif 'dataloader' in stgan.other_state_dicts.keys():\n    dataloader.set_state(stgan.other_state_dicts['dataloader'])\n    stgan.logger.debug(f'Loaded dataloader state! Current pem_index={dataloader.get_state()[\"perm_index\"]}')\n\n# FIX: Change batch size (if needed)\nstgan.update_batch_size(batch_size, sampler_instance=dataloader.sampler)\n\n###################################\n###       Training Loop         ###\n###################################\n#   - get the correct tqdm instance\nexec_tqdm = get_tqdm()\n#   - start training loop from last checkpoint's epoch and step\ntorch.cuda.empty_cache()\ngcapture_ready = True\nasync_results = None\nstgan.logger.info(f'[training loop] STARTING (epoch={stgan.epoch}, step={stgan.initial_step})')\nfor epoch in range(stgan.epoch, n_epochs):\n    # Check if the networks should grow\n    if stgan.growing() or batch_size != stgan.current_batch_size:\n        batch_size = stgan.current_batch_size\n        stgan.logger.critical(f'Reinitializing Dataloader... (new batch_size={batch_size})')\n        dataloader = dataloader.update_batch_size(batch_size=batch_size)\n        stgan.update_batch_size(batch_size, sampler_instance=dataloader.sampler)\n    \n    # Set steps\n    display_step = display_steps[stgan.gen.resolution]\n    checkpoint_step = checkpoint_steps[stgan.gen.resolution]\n    metrics_step = metrics_steps[stgan.gen.resolution]\n    \n    # noinspection PyProtectedMember\n    d = {\n        'step': stgan.step,\n        'initial_step': stgan.initial_step,\n        'epoch': stgan.epoch,\n        '_counter': stgan._counter,\n        'epoch_inc': stgan.epoch_inc,\n    }\n    # initial_step = stgan.initial_step % len(dataloader)\n    stgan.logger.debug('[START OF EPOCH] ' + str(d))\n\n    # Instantiate progress bart\n    progress_bar: tqdm = exec_tqdm(dataloader, initial=stgan.initial_step)\n    progress_bar.set_description(f'[e {str(epoch).zfill(3)}/{str(n_epochs).zfill(3)}]' +\n                                 f'[g --.-- | d --.--]')\n\n    real: Tensor\n    for real in progress_bar:\n        # Downsample images\n        if real.shape[-1] != stgan.gen.resolution:\n            real = transforms.Resize(size=stgan.gen.resolution, interpolation=Image.BILINEAR)(real)\n\n        # Transfer image batches to GPU\n        real = real.to(exec_device)\n\n        # Perform a forward + backward pass + weight update on the Generator & Discriminator models\n        disc_loss, gen_loss = stgan(real)\n\n        # Update loss in tqdm description\n        if gen_loss is not None and disc_loss is not None:\n            progress_bar.set_description(f'[e {str(epoch).zfill(3)}/{str(n_epochs).zfill(3)}]' +\n                                         f'[g {round(gen_loss.item(), 2)} | d {round(disc_loss.item(), 2)}]')\n\n        # Metrics & Checkpoint Code\n        if stgan.step % checkpoint_step == 0:\n            # Check if another upload is pending\n            if not gcapture_ready and async_results:\n                # Wait for previous upload to finish\n                stgan.logger.warning('Waiting for previous gcapture() to finish...')\n                [r.wait() for r in async_results]\n                stgan.logger.warning('DONE! Starting new capture now.')\n            # Capture current model state, including metrics and visualizations\n            async_results = stgan.gcapture(checkpoint=True, metrics=stgan.step % metrics_step == 0, visualizations=True,\n                                           dataloader=dataloader, in_parallel=True, show_progress=True,\n                                           delete_after=True)\n        # Visualization code\n        elif stgan.step % display_step == 0:\n            visualization_img = stgan.visualize()\n            visualization_img.show() if not in_notebook() else display(visualization_img)\n\n        # Check if a pending checkpoint upload has finished\n        if async_results:\n            gcapture_ready = all([r.ready() for r in async_results])\n            if gcapture_ready:\n                stgan.logger.info(f'gcapture() finished')\n                if stgan.latest_checkpoint_had_metrics:\n                    stgan.logger.info(str(stgan.latest_metrics))\n                async_results = None\n\n        # If run locally one pass is enough\n        if run_locally and gcapture_ready:\n            break\n\n    # If run locally one pass is enough\n    if run_locally:\n        break\n\n    # noinspection PyProtectedMember\n    d = {\n        'step': stgan.step,\n        'initial_step': stgan.initial_step,\n        'epoch': stgan.epoch,\n        '_counter': stgan._counter,\n        'epoch_inc': stgan.epoch_inc,\n    }\n    stgan.logger.debug('[END OF EPOCH] ' + str(d))\n\n# Check if a pending checkpoint exists\nif async_results:\n    ([r.wait() for r in async_results])\n    stgan.logger.info(f'last gcapture() finished')\n    if stgan.latest_checkpoint_had_metrics:\n        stgan.logger.info(str(stgan.latest_metrics))\n    async_results = None\n\n# Training finished!\nstgan.logger.info('[training loop] DONE')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T19:18:23.021787Z","iopub.execute_input":"2022-03-07T19:18:23.022052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -f '/kaggle/working/GoogleDrive/Models/model_name=stylegan_karras/Configurations/karrasA_z512_16gb.yaml'\nmodels_groot.subfolder_by_name('model_name=stylegan_karras').subfolder_by_name('Configurations').download('karrasA_z512_16gb.yaml')\nmodels_groot.subfolder_by_name('model_name=stylegan_karras').subfolder_by_name('Configurations').file_by_name('karrasA_z512_16gb.yaml').path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat '/kaggle/working/GoogleDrive/Models/model_name=stylegan_karras/Configurations/karrasA_z512_16gb.yaml'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stgan.gen.locals['truncation'] = 0.9\n# img = stgan.visualize_indices(indices=[0, 1, 2])\n# torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Evaluate StyleGAN\nIn this section we evaluate the generation performance of our trained network using the SOTA GAN evaluation metrics.\n\n## 3.1) Get the metrics evolution plots\nWe plot how the metrics evolved during training. The GAN is **not** trained to minimize those metrics (they are\ncalculated using `torch.no_grad()`) and thus this evolution merely depends on the network and showcases the correlation\nbetween the GAN evaluation metrics, and the losses (e.g. adversarial & reconstruction) used to optimize the network.","metadata":{}},{"cell_type":"code","source":"# Since the StyleGAN implements utils.ifaces.Visualizable, we can\n# directly call visualize_metrics() on the model instance.\n_ = stgan.visualize_metrics(upload=True, preview=True)\n\n# Visualize losses (gen & disc in the same plot)\n_ = stgan.visualize_losses(dict_keys=(('gen_loss', 'disc_loss'), ), upload=True, preview=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2) Evaluate Generated Samples\nIn order to evaluate generated samples and compare model with other GAN architectures trained on the same dataset.\nFor this purpose we will re-calculate the evaluation metrics as stated above, but with a much bigger number of samples.\nIn this way, the metrics will be more trustworthy and comparable with the corresponding metrics in the original paper.","metadata":{}},{"cell_type":"code","source":"# Initialize a new evaluator instance\n# (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\nevaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataloader.test_set,\n                         z_dim=z_dim, n_samples=10000, batch_size=metrics_batch_size,\n                         device=exec_device, f1_k=f1_k, ssim_c_img=target_channels)\n\n# Run the evaluator\nmetrics_dict = evaluator.evaluate(gen=stgan.gen, metric_name='all', show_progress=True)\n\n# Print results\nimport json\n\nprint(json.dumps(metrics_dict, indent=4))\n\n#\n#------------------------------\n# Test Set Metrics (epoch=83)\n#-----------------------------\n#\n# {\n#     \"fid\": 0.0,\n#     \"is\": 0.0,\n#     \"f1\": 0.0,\n#     \"precision\": 0.0,\n#     \"recall\": 0.0,\n#     \"ssim\": 0.0\n# }\n#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a new evaluator instance\n# (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\nevaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataloader.dataset, z_dim=z_dim,\n                         n_samples=10000, batch_size=metrics_batch_size, device=exec_device, f1_k=f1_k,\n                         ssim_c_img=target_channels)\n\n# Run the evaluator\nmetrics_dict = evaluator.evaluate(gen=stgan.gen, metric_name='all', show_progress=True)\n\n# Print results\nimport json\n\nprint(json.dumps(metrics_dict, indent=4))\n\n#\n#----------------------------------\n# Training Set Metrics (epoch=83)\n#---------------------------------\n#\n# {\n#     \"fid\": 0.0,\n#     \"is\": 0.0,\n#     \"f1\": 0.0,\n#     \"precision\": 0.0,\n#     \"recall\": 0.0,\n#     \"ssim\": 0.0\n# }\n#","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}