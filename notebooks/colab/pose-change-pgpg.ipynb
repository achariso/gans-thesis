{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Mount drive, unzip data, clone repo, install packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1) Mount Drive and define paths\n",
    "Run provided colab code to mount Google Drive. Then define dataset paths relative to mount point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -rf /content/sample_data\n",
    "!rm -rf /content/*.jpg\n",
    "!rm -rf /content/*.png\n",
    "!rm -rf /content/*.json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# noinspection PyUnresolvedReferences,PyPackageRequirements\n",
    "from google.colab import drive\n",
    "mount_root_abs = '/content/drive'\n",
    "drive.mount(mount_root_abs)\n",
    "drive_root = f'{mount_root_abs}/MyDrive'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "# DeepFashion In-shop Clothes Retrieval Benchmark (ICRB)\n",
    "df_root_drive = f'{drive_root}/Datasets/DeepFashion'\n",
    "assert os.path.exists(df_root_drive)\n",
    "df_icrb_root_drive = f'{df_root_drive}/In-shop Clothes Retrieval Benchmark'\n",
    "assert os.path.exists(df_icrb_root_drive)\n",
    "df_icrb_img_zip_abs_drive = f'{df_icrb_root_drive}/Img.zip'\n",
    "\n",
    "# If Img.zip is not present, we need to unzip .../Img/img_iuv.zip directory\n",
    "# from drive root and then run ICRBScraper.run() from /src/dataset/deep_fashion.\n",
    "# For this nb, we skip this since it'll take an eternity to complete with\n",
    "# mounted Google Drive.\n",
    "assert os.path.exists(df_icrb_img_zip_abs_drive), \\\n",
    "  'Please upload a processed zip (processing img.zip in colab will take' + \\\n",
    "  f' for AGES). \\nTried: {df_icrb_img_zip_abs_drive}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2) Unzip Img directory in Colab\n",
    "By unzipping the Img_processed.zip in Colab before running our model we gain significant disk reading speedups.\n",
    "So, the first step is to unzip images directory, and then save the image directory before proceeding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_icrb_root = df_icrb_root_drive.replace(drive_root, '/content/data')\n",
    "df_icrb_img_root = f'{df_icrb_root}/Img'\n",
    "if not os.path.exists(df_icrb_img_root):\n",
    "  # Clear any previous attempts\n",
    "  # ATTENTION: This will remove /contents/data/*. So, before running, please make\n",
    "  # sure no usable files will be deleted.\n",
    "  !mkdir -p /content/data\n",
    "  !rm -rf /content/data\n",
    "\n",
    "  # Create output directory\n",
    "  !mkdir -p \"$df_icrb_root\"\n",
    "\n",
    "  # Transfer Img.zip from Google Drive to Colab\n",
    "  df_icrb_img_zip_abs = f'{df_icrb_root}/{os.path.basename(df_icrb_img_zip_abs_drive)}'\n",
    "  if not os.path.exists(df_icrb_img_zip_abs):\n",
    "    !cp \"$df_icrb_img_zip_abs_drive\" \"$df_icrb_root\"\n",
    "  # Unzip it in Colab\n",
    "  !unzip -q \"$df_icrb_img_zip_abs\" -d \"$df_icrb_root\"\n",
    "  # Handle newly-created image directory\n",
    "  assert os.path.exists(df_icrb_img_root), f'df_icrb_img_root: {df_icrb_img_root}'\n",
    "  assert not os.path.exists(f'{df_icrb_img_root}/Img')\n",
    "  assert not os.path.exists(f'{df_icrb_img_root}/img')\n",
    "  !rm -f \"$df_icrb_img_zip_abs\"\n",
    "  assert not os.path.exists(df_icrb_img_zip_abs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3) Clone github repo\n",
    "Clone achariso/gans-thesis repo into /content/code\n",
    " using git clone.\n",
    " For more info see: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "repo_root = '/content/code/gans-thesis'\n",
    "if not os.path.exists(repo_root) and not os.path.exists(f'{repo_root}/requirements.txt'):\n",
    "    # Check that ssh keys exist\n",
    "    assert os.path.exists(f'{drive_root}/GitHub Keys')\n",
    "    id_rsa_abs_drive = f'{drive_root}/GitHub Keys/id_rsa'\n",
    "    id_rsa_pub_abs_drive = f'{id_rsa_abs_drive}.pub'\n",
    "    assert os.path.exists(id_rsa_abs_drive)\n",
    "    assert os.path.exists(id_rsa_pub_abs_drive)\n",
    "    # On first run: Add ssh key in repo\n",
    "    if not os.path.exists('/root/.ssh'):\n",
    "        # Transfer config file\n",
    "        ssh_config_abs_drive = f'{drive_root}/GitHub Keys/config'\n",
    "        assert os.path.exists(ssh_config_abs_drive)\n",
    "        !mkdir -p ~/.ssh\n",
    "        !cp -f \"$ssh_config_abs_drive\" ~/.ssh/\n",
    "        # # Add github.com to known hosts\n",
    "        !ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
    "        # Test: !ssh -T git@github.com\n",
    "\n",
    "    # Remove any previous attempts\n",
    "    !rm -rf \"$repo_root\"\n",
    "    !mkdir -p \"$repo_root\"\n",
    "    # Clone repo\n",
    "    !git clone git@github.com:achariso/gans-thesis.git \"$repo_root\"\n",
    "    src_root = f'{repo_root}/src'\n",
    "    !rm -rf \"$repo_root\"/report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4) Install pip packages\n",
    "All required files are stored in a requirements.txt files at the repository's root.\n",
    "Use `pip install -r requirements.txt` from inside the dir to install required packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd $repo_root\n",
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5) Add code/, */src/ to path\n",
    "This is necessary in order to be able to run the modules."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "content_root_abs = f'{repo_root}'\n",
    "src_root_abs = f'{repo_root}/src'\n",
    "%env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Test code\n",
    "Test that pulled code is running by running its tests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix: Adjust figure size for better plotting in Colab\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run src/main.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_root = f'{repo_root}/tests'\n",
    "!python -m unittest discover -s \"$test_root\" -t \"$test_root\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Train PGPG model on DeepFashion\n",
    "In this section we give the actual training code for PGPG network. PGPG consists of a 2-stage generator, where each stage is a UNET-like model, and, in our version, a PatchGAN discriminator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# noinspection PyProtectedMember\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1) Hyper-parameters settings\n",
    "In this sub-section we define some important training hyper-parameters such as the batch size, and the number of epochs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training hyperparams\n",
    "n_epochs = 100\n",
    "batch_size = 48\n",
    "lr = 1e-4\n",
    "adv_criterion = nn.MSELoss()\n",
    "recon_criterion = nn.L1Loss()\n",
    "\n",
    "load_shape = 256\n",
    "load_channels = 3\n",
    "target_shape = 128\n",
    "target_channels = 3\n",
    "\n",
    "train_test_splits = [90, 10]    # for a 90% training - 10% evaluation set split\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# Set save_model to True to have training loop crate model checkpoints at a\n",
    "# certain interval\n",
    "save_model = True\n",
    "\n",
    "# Set the desired LR schedulers for Generator and Discriminator optimizers\n",
    "# Supported types: 'on_plateau', 'cyclic'\n",
    "gen_lr_scheduler_type = None\n",
    "disc_lr_scheduler_type = None\n",
    "\n",
    "# Step listeners\n",
    "display_step = 200\n",
    "checkpoint_step = 500\n",
    "\n",
    "model_chkpts_root_drive = '/content/drive/MyDrive/Model Checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2) Data\n",
    "In this sub-section we define the dataset & the dataloader to use. In particular, we use DeepFashion's In-shop Clothes Retrieval Benchmark (ICRB) in cross-pose mode. As so, at each iteration dataloader yields 3 images:\n",
    "\n",
    "\n",
    "*   `image_1`: the input (condition) image to the generator (from which the styles, clothes, etc. will be \"fetched\")\n",
    "*   `image_2`: `image_1` at a different pose and/or scale (from which the output \"body structure\" and \"position\" will be \"fetched\")\n",
    "*   `dense_pose_2`: the dense pose annotation (as an RGB image) of `image_2`\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataset.deep_fashion import ICRBCrossPoseDataloader, ICRBDataset\n",
    "from utils.pytorch import invert_transforms\n",
    "\n",
    "# Image transforms:\n",
    "# If target_shape is different from load one, resize & crop. If target_shape is\n",
    "# different from load shape, convert to grayscale.\n",
    "# Update: Now done automatically if you set target_channels, target_shape when\n",
    "# instantiating the dataloader\n",
    "#   - len(dataloader) = <number of batches>\n",
    "#   - len(dataloader.dataset) = <number of total dataset items>\n",
    "norm_mean = norm_std = 0.5 # params of transforms.Normalize()\n",
    "gen_transforms = ICRBDataset.get_image_transforms(target_shape, target_channels,\n",
    "                                                  norm_mean=norm_mean,\n",
    "                                                  norm_std=norm_std)\n",
    "# Save inverse transform\n",
    "gen_transforms_inv = invert_transforms(gen_transforms)\n",
    "\n",
    "# Define dataloader\n",
    "skip_pose_norm = True\n",
    "dl = ICRBCrossPoseDataloader(batch_size=batch_size,\n",
    "                             image_transforms=gen_transforms,\n",
    "                             skip_pose_norm=skip_pose_norm,\n",
    "                             splits=train_test_splits,\n",
    "                             pin_memory=True)\n",
    "# Apply basic tests\n",
    "assert issubclass(dl.__class__, DataLoader)\n",
    "assert len(dl) == len(dl.dataset) // batch_size + \\\n",
    "                              (1 if len(dl.dataset) % batch_size else 0)\n",
    "_image_1, _image_2, _dense_pose_2 = next(iter(dl))\n",
    "assert tuple(_image_1.shape) == (batch_size, target_channels, target_shape, target_shape)\n",
    "assert tuple(_image_2.shape) == (batch_size, target_channels, target_shape, target_shape)\n",
    "assert tuple(_dense_pose_2.shape) == (batch_size, target_channels, target_shape, target_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3) Define PGPG model\n",
    "In this sub-section we define the PGPG's children network models and in particular:\n",
    "\n",
    "\n",
    "*   the stage-I generator, `G1`: UNET-like with skip connections, and an FC layer at the bottleneck\n",
    "*   the stage-II generator, `G2`: UNET-like with skip connections and Dropout at the first half of the \"encoder\" part of the UNET-like model\n",
    "*   the whole generator, `G`: composed of the two stages\n",
    "*   the discriminator, `D`: PatchGAN discriminator\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.discriminators.patch_gan import PatchGANDiscriminator\n",
    "from modules.generators.pgpg import PGPGGenerator\n",
    "from utils.train import get_adam_optimizer, weights_init_naive, load_model_chkpt, get_optimizer_lr_scheduler\n",
    "\n",
    "# Define models\n",
    "gen = PGPGGenerator(c_in=2*target_channels, c_out=target_channels,\n",
    "                    w_in=target_shape, h_in=target_shape).to(device)\n",
    "disc = PatchGANDiscriminator(c_in=2*target_channels,\n",
    "                             # NOTE: for 5 contracting blocks, output is 4x4\n",
    "                             n_contracting_blocks=5,\n",
    "                             use_spectral_norm=True).to(device)\n",
    "# Define optimizers\n",
    "# Note: Both generators, G1 & G2, are trained using a joint optimizer\n",
    "gen_opt = get_adam_optimizer(gen, lr=lr, betas=(0.9, 0.999))\n",
    "disc_opt = get_adam_optimizer(disc, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Initialize weights\n",
    "gen = gen.apply(weights_init_naive)\n",
    "disc = disc.apply(weights_init_naive)\n",
    "\n",
    "# Load checkpoints\n",
    "if save_model:\n",
    "    chkpt_images = 0\n",
    "    state_dict = None\n",
    "    try:\n",
    "        chkpt_images, state_dict = load_model_chkpt(model=disc, model_name='pgpg', dict_key='disc', model_opt=disc_opt)\n",
    "        print('Loaded Discriminator checkpoint!')\n",
    "    except AssertionError as e:\n",
    "        print(f'Error while loading Discriminator checkpoint: {str(e)}')\n",
    "\n",
    "    try:\n",
    "        if state_dict:\n",
    "            load_model_chkpt(model=gen, model_name='pgpg', dict_key='gen', model_opt=gen_opt, state_dict=state_dict)\n",
    "        else:\n",
    "            chkpt_images, state_dict = load_model_chkpt(model=gen, model_name='pgpg', dict_key='gen', model_opt=gen_opt)\n",
    "        chkpt_epoch, chkpt_images_in_current_epoch = divmod(chkpt_images, batch_size * len(dl))\n",
    "        chkpt_step = chkpt_images_in_current_epoch // batch_size\n",
    "        print('Loaded Generator checkpoint!')\n",
    "        print(f'\\t--> chkpt_images={chkpt_images}, chkpt_epoch={chkpt_epoch}, chkpt_step={chkpt_step}')\n",
    "    except AssertionError as e:\n",
    "        print(f'Error while loading Generator checkpoint: {str(e)}')\n",
    "        chkpt_epoch = chkpt_step = 0\n",
    "\n",
    "    # Load Dataloader sampler state\n",
    "    if state_dict and 'dataloader' in state_dict.keys():\n",
    "        dl.set_state(state_dict['dataloader'])\n",
    "\n",
    "else:\n",
    "    chkpt_epoch = chkpt_step = 0\n",
    "\n",
    "# Learning-rate Schedulers\n",
    "if gen_lr_scheduler_type:\n",
    "    if gen_lr_scheduler_type == 'cyclic':\n",
    "        gen_opt_lr_scheduler = get_optimizer_lr_scheduler(gen_opt, schedule_type=gen_lr_scheduler_type,\n",
    "                                                          base_lr=lr, max_lr=10 * lr, step_size_up=2 * len(dl),\n",
    "                                                          mode='exp_range', gamma=0.9,\n",
    "                                                          cycle_momentum=False,\n",
    "                                                          last_epoch=chkpt_epoch if 'initial_lr' in gen_opt.param_groups[0].keys() else -1)\n",
    "    else:\n",
    "        gen_opt_lr_scheduler = get_optimizer_lr_scheduler(gen_opt, schedule_type=gen_lr_scheduler_type)\n",
    "else:\n",
    "    gen_opt_lr_scheduler = None\n",
    "if disc_lr_scheduler_type:\n",
    "    if disc_lr_scheduler_type == 'cyclic':\n",
    "        disc_opt_lr_scheduler = get_optimizer_lr_scheduler(disc_opt, schedule_type=disc_lr_scheduler_type,\n",
    "                                                           base_lr=lr, max_lr=10 * lr, step_size_up=2 * len(dl),\n",
    "                                                           mode='exp_range', gamma=0.9,\n",
    "                                                           cycle_momentum=False,\n",
    "                                                           last_epoch=chkpt_epoch if 'initial_lr' in disc_opt.param_groups[0].keys() else -1)\n",
    "    else:\n",
    "        disc_opt_lr_scheduler = get_optimizer_lr_scheduler(disc_opt, schedule_type=disc_lr_scheduler_type)\n",
    "else:\n",
    "    disc_opt_lr_scheduler = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4) Evaluation Metrics\n",
    "The evaluation metrics that we will use to evaluate model performance during an after training are:\n",
    "\n",
    "* Frechet Inception Distance, FID\n",
    "* Inception Score, IS\n",
    "* Structural Similarity Index, (SSIM)\n",
    "* Precision, Recall and F1, (P, R, F1)\n",
    "\n",
    "These metrics are saved on each model checkpoint. Below, we initialize the calculators for these metrics.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataset.deep_fashion import ICRBCrossPoseDataset, ICRBDataset\n",
    "\n",
    "from utils.metrics.fid import FID\n",
    "from utils.metrics.is_ import IS\n",
    "from utils.metrics.f1 import F1\n",
    "from utils.metrics.ssim import SSIM\n",
    "\n",
    "metrics_n_sample = 512\n",
    "metrics_batch_sample = 8\n",
    "\n",
    "metrics_gen_transforms = ICRBDataset.get_image_transforms(target_shape=target_shape, target_channels=target_channels)\n",
    "metrics_dataset = ICRBCrossPoseDataset(image_transforms=None, pose=True)\n",
    "metrics_dataset_with_transforms = ICRBCrossPoseDataset(image_transforms=metrics_gen_transforms, pose=True)\n",
    "\n",
    "fid_calculator = FID(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "is_calculator = IS(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "f1_calculator = F1(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "ssim_calculator = SSIM(n_samples=metrics_n_sample, batch_size=metrics_batch_sample,\n",
    "                       c_img=target_channels, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5) PGPG Training\n",
    "Finally, let's start training the model! In the following cells we also define methods to print images to be able to preview inference (i.e. generation) quality evolution during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "cur_step = chkpt_step + chkpt_epoch * len(dl)\n",
    "first_loop = True\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(chkpt_epoch, n_epochs):\n",
    "    for image_1, image_2, pose_2 in tqdm(dl, initial=chkpt_step):\n",
    "        cur_batch_size = len(image_1)\n",
    "\n",
    "        image_1 = image_1.to(device)\n",
    "        image_2 = image_2.to(device)\n",
    "        pose_2 = pose_2.to(device)\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        ########   Update Discriminator   ########\n",
    "        ##########################################\n",
    "        disc_opt.zero_grad()                    # Zero out gradient before backprop\n",
    "        with torch.no_grad():\n",
    "            _, g_out = gen(image_1, pose_2)\n",
    "        disc_loss = disc.get_loss(real=image_2, fake=g_out, condition=image_1,\n",
    "                                  criterion=adv_criterion)\n",
    "        disc_loss.backward(retain_graph=True)   # Update discriminator gradients\n",
    "        disc_opt.step()                         # Update discriminator weights\n",
    "        # Update LR (if needed)\n",
    "        if disc_lr_scheduler_type and disc_opt_lr_scheduler:\n",
    "            disc_opt_lr_scheduler.step(metrics=disc_loss) if disc_lr_scheduler_type == 'on_plateau' \\\n",
    "                else disc_opt_lr_scheduler.step()\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        ########     Update Generator     ########\n",
    "        ##########################################\n",
    "        gen_opt.zero_grad()\n",
    "        g1_loss, g2_loss, g1_out, g_out = gen.get_loss(x=image_1, y=image_2,\n",
    "                    y_pose=pose_2.clone(), disc=disc, adv_criterion=adv_criterion,\n",
    "                    recon_criterion=recon_criterion)\n",
    "        gen_loss = g1_loss + g2_loss\n",
    "        gen_loss.backward()                     # Update generator gradients\n",
    "\n",
    "        gen_opt.step()                          # Update generator optimizer\n",
    "        # Update LR (if needed)\n",
    "        if gen_lr_scheduler_type and gen_opt_lr_scheduler:\n",
    "            gen_opt_lr_scheduler.step(metrics=gen_loss) if gen_lr_scheduler_type == 'on_plateau' \\\n",
    "                else gen_opt_lr_scheduler.step()\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        ########      Visualizations      ########\n",
    "        ##########################################\n",
    "        # Keep track of the average losses\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        # Visualization code\n",
    "        if cur_step % display_step == 0:\n",
    "            print(f\"\\n\\t--> Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "\n",
    "            g1_out_first = gen_transforms_inv(g1_out[0].detach().cpu()).float()\n",
    "            g1_out_last = gen_transforms_inv(g1_out[-1].detach().cpu()).float()\n",
    "            g_out_first = gen_transforms_inv(g_out[0].detach().cpu()).float()\n",
    "            g_out_last = gen_transforms_inv(g_out[-1].detach().cpu()).float()\n",
    "            g2_out_fist = g_out_first - g1_out_first\n",
    "            g2_out_last = g_out_last - g1_out_last\n",
    "\n",
    "            image_1_first = gen_transforms_inv(image_1[0].cpu())\n",
    "            pose_2_first = pose_2[0].cpu() # No normalization since skip_pose_norm = True\n",
    "            image_2_first = gen_transforms_inv(image_2[0].cpu())\n",
    "            image_1_last = gen_transforms_inv(image_1[-1].cpu())\n",
    "            pose_2_last = pose_2[-1].cpu() # No normalization since skip_pose_norm = True\n",
    "            image_2_last = gen_transforms_inv(image_2[-1].cpu())\n",
    "\n",
    "            cat_images = torch.cat((image_1_first, pose_2_first, image_2_first, g1_out_first, g2_out_fist, g_out_first), dim=2)\n",
    "            cat_images = cat_images.detach().cpu()\n",
    "            if target_channels == 1:\n",
    "              cat_images = cat_images.view([cat_images.shape[1], cat_images.shape[2]])\n",
    "            else:\n",
    "              cat_images = cat_images.permute(1, 2, 0)\n",
    "            plt.imshow(cat_images, cmap='gray' if target_channels == 1 else None)\n",
    "            plt.show()\n",
    "\n",
    "            cat_images = torch.cat((image_1_last, pose_2_last, image_2_last, g1_out_last, g2_out_last, g_out_last), dim=2)\n",
    "            cat_images = cat_images.detach().cpu()\n",
    "            if target_channels == 1:\n",
    "              cat_images = cat_images.view([cat_images.shape[1], cat_images.shape[2]])\n",
    "            else:\n",
    "              cat_images = cat_images.permute(1, 2, 0)\n",
    "            plt.imshow(cat_images, cmap='gray' if target_channels == 1 else None)\n",
    "            plt.show()\n",
    "\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        ########   Metrics & Checkpoint   ########\n",
    "        ##########################################\n",
    "        if save_model and not first_loop and cur_step % checkpoint_step == 0:\n",
    "            # Check if checkpoint already exists\n",
    "            new_chkpt_path = f'{model_chkpts_root_drive}/pgpg_{str(cur_step).zfill(10)}_{batch_size}.pth'\n",
    "            if not os.path.exists(new_chkpt_path):\n",
    "                # Calculate metrics\n",
    "                gen = gen.eval()\n",
    "                with torch.no_grad():\n",
    "                    metrics_fid = fid_calculator(metrics_dataset, gen=gen, gen_transforms=metrics_gen_transforms,\n",
    "                                                target_index=1, condition_indices=(0, 2))\n",
    "                    metrics_is = is_calculator(metrics_dataset, gen=gen, gen_transforms=metrics_gen_transforms,\n",
    "                                                target_index=1, condition_indices=(0, 2))\n",
    "                    metrics_f1, metrics_p, metrics_r = f1_calculator(metrics_dataset, gen=gen,\n",
    "                                                                    gen_transforms=metrics_gen_transforms,\n",
    "                                                                    target_index=1, condition_indices=(0, 2))\n",
    "                    metrics_ssim = ssim_calculator(metrics_dataset_with_transforms, gen=gen,\n",
    "                                                  target_index=1, condition_indices=(0, 2),\n",
    "                                                  skip_asserts=True)\n",
    "                    metrics_dict = {\n",
    "                        'fid': metrics_fid.item(),\n",
    "                        'is': metrics_is.item(),\n",
    "                        'f1': metrics_f1.item(),\n",
    "                        'precision': metrics_p.item(),\n",
    "                        'recall': metrics_r.item(),\n",
    "                        'ssim': metrics_ssim.item()\n",
    "                    }\n",
    "                print(metrics_dict)\n",
    "                # Save state dicts alongside metrics in a single .pth file\n",
    "                torch.save({\n",
    "                    'gen': gen.state_dict(),\n",
    "                    'gen_opt': gen_opt.state_dict(),\n",
    "                    'disc': disc.state_dict(),\n",
    "                    'disc_opt': disc_opt.state_dict(),\n",
    "                    'metrics': metrics_dict,\n",
    "                    'dataloader': dl.get_state(),\n",
    "                }, new_chkpt_path)\n",
    "                gen = gen.train()\n",
    "\n",
    "        cur_step += 1\n",
    "        first_loop = False\n",
    "    chkpt_step = 0\n",
    "\n",
    "#####################################\n",
    "#### Cyclic LR started @ step 29K ###\n",
    "#####################################\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preview metrics\n",
    "fids = []\n",
    "iss = []\n",
    "f1s = []\n",
    "ssims = []\n",
    "for _root, _, _files in os.walk(model_chkpts_root_drive):\n",
    "    for _f in _files:\n",
    "        if not _f.startswith('pgpg'):\n",
    "            continue\n",
    "\n",
    "        metrics = torch.load(f'{_root}/{_f}')['metrics']\n",
    "        fids.append(metrics['fid'])\n",
    "        iss.append(metrics['is'])\n",
    "        f1s.append(metrics['f1'])\n",
    "        ssims.append(metrics['ssim'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "plt.plot(fids); plt.title('FID'); plt.show()\n",
    "plt.plot(iss); plt.title('IS'); plt.show()\n",
    "plt.plot(ssims); plt.title('SSIM'); plt.show()\n",
    "plt.plot(f1s); plt.title('F1'); plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5) Evaluate Generated Samples\n",
    "In order to evaluate generated samples and compare model with other GAN architectures trained on the same dataset. For this purpose we will re-calculate the evaluation metrics as stated above, but with a much bigger number of samples. In this way, the metrics will be more trustworthy and comparable with the corresponding metrics in the original paper.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate final metrics\n",
    "metrics_n_sample = 10000\n",
    "metrics_batch_sample = 64\n",
    "\n",
    "fid_calculator = FID(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "is_calculator = IS(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "f1_calculator = F1(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device)\n",
    "ssim_calculator = SSIM(n_samples=metrics_n_sample, batch_size=metrics_batch_sample, device=device, c_img=target_channels)\n",
    "\n",
    "gen = gen.eval()\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    metrics_fid = fid_calculator(metrics_dataset, gen=gen, gen_transforms=metrics_gen_transforms,\n",
    "                                 target_index=1, condition_indices=(0, 2))\n",
    "    metrics_is = is_calculator(metrics_dataset, gen=gen, gen_transforms=metrics_gen_transforms,\n",
    "                                 target_index=1, condition_indices=(0, 2))\n",
    "    metrics_f1, metrics_p, metrics_r = f1_calculator(metrics_dataset, gen=gen,\n",
    "                                                     gen_transforms=metrics_gen_transforms,\n",
    "                                                     target_index=1, condition_indices=(0, 2),\n",
    "                                                     row_batch_size=16, col_batch_size=16)\n",
    "    metrics_ssim = ssim_calculator(metrics_dataset_with_transforms, gen=gen,\n",
    "                                   target_index=1, condition_indices=(0, 2),\n",
    "                                   skip_asserts=True)\n",
    "\n",
    "print(str({\n",
    "    'fid': metrics_fid.item(),\n",
    "    'is': metrics_is.item(),\n",
    "    'f1': metrics_f1.item(), 'precision': metrics_p.item(), 'recall': metrics_r.item(),\n",
    "    'ssim': metrics_ssim.item()\n",
    "}))\n",
    "\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "##                                  ##\n",
    "## At epoch=3, the metrics were:    ##\n",
    "## {                                ##\n",
    "##   \"fid\": 283.1460876464844       ##\n",
    "##   \"is\": 3.959059476852417        ##\n",
    "##   \"f1\": 0.2747354805469513       ##\n",
    "##   \"precision\": 0.18958996236324  ##\n",
    "##   \"recall\": 0.49870622158050537  ##\n",
    "##   \"ssim\": 0.6973406672477722     ##\n",
    "## }                                ##\n",
    "##                                  ##\n",
    "######################################\n",
    "######################################\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}