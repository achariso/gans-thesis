{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbPe-ZvX-NkH"
   },
   "source": [
    "# 1) Mount drive, unzip data, clone repo, install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AeDFep2-NkR"
   },
   "source": [
    "## 1.1) Mount Drive and define paths\n",
    "Run provided colab code to mount Google Drive. Then define dataset paths relative to mount point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drive_root = f'/home/achariso/PycharmProjects/gans-thesis/.gdrive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LsbQGZcN-NkS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Handbags-to-Shoes Dataset\n",
    "h2s_root_drive = f'{drive_root}/Datasets/Bags2Shoes'\n",
    "assert os.path.exists(h2s_root_drive)\n",
    "h2s_img_zip_abs_drive = f'{h2s_root_drive}/handbags_64.hdf5'\n",
    "\n",
    "# Test if processed hdf5 file exists in dataset root\n",
    "assert os.path.exists(h2s_img_zip_abs_drive), f'Please upload the dataset files (handbags_64.hdf5, shoes_64.hdf5) ' + \\\n",
    "                                              f'in Google Drive. \\nTried: {h2s_img_zip_abs_drive}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntYgqTyp-NkT"
   },
   "source": [
    "## 1.2) Clone GitHub repo\n",
    "Clone achariso/gans-thesis repo into /content/code using git clone.\n",
    "For more info see: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g1lIuIOm-NkT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo_root = '/home/achariso/PycharmProjects/gans-thesis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhZWMWMu-NkU"
   },
   "source": [
    "## 1.4) Add code/, */src/ to path\n",
    "This is necessary in order to be able to run the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gq1Tph8v-NkU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=\"/env/python:/home/achariso/PycharmProjects/gans-thesis:/home/achariso/PycharmProjects/gans-thesis/src\"\n"
     ]
    }
   ],
   "source": [
    "content_root_abs = f'{repo_root}'\n",
    "src_root_abs = f'{repo_root}/src'\n",
    "%env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy6DRL0S-NkU"
   },
   "source": [
    "# 2) Train CycleGAN model on Bags2Shoes_64 dataset\n",
    "In this section we run the actual training loop for CycleGAN network. CycleGAN consists of two cross-domain generators\n",
    "and, in our version, two PatchGAN discriminators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N0L9AAV-NkV"
   },
   "source": [
    "### Colab Bug Workaround\n",
    "Bug: matplotlib cache not rebuilding.\n",
    "Solution: Run the following code and then restart the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aFgK5mX5-NkV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now inside train_ccgan.py\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLpuOKli-NkV"
   },
   "source": [
    "### Actual Run\n",
    "Eventually, run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q3v3ab5x-NkV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/achariso/PycharmProjects/gans-thesis\n"
     ]
    }
   ],
   "source": [
    "%cd \"$repo_root\"\n",
    "\n",
    "chkpt_step = None       # supported: 'latest', <int>, None\n",
    "log_level = 'debug'     # supported: 'debug', 'info', 'warning', 'error', 'critical', 'fatal'\n",
    "device = 'cpu'          # supported: 'cpu', 'cuda', 'cuda:<GPU_INDEX>'\n",
    "\n",
    "# Running with -i enables us to get variables defined inside the script (the script runs inline)\n",
    "%run -i src/train_setup.py --log_level $log_level --chkpt_step $chkpt_step --seed 42 --device $device --run_locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwqTPmw9-NkV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CycleGAN Training\n",
    "\n",
    "Setup/preparation before starting CycleGAN training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wfgy14_N-NkW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mFound 138.8K total images in the handbags_64 dataset\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mFound 50K total images in the shoes_64 dataset\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mDataset's length (max): 138.8K images\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mSplit dataset: len(training_set)=124.9K, len(test_set)=13.9K\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mDataset is fetched and unzipped!\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/achariso/PycharmProjects/gans-thesis/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mUsing device: cpu\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mModel initialized. Number of params = 36.1M\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mModel warmed-up (internal counters).\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%cd 'src/'\n",
    "\n",
    "import torch\n",
    "from IPython.core.display import display\n",
    "from torch import Tensor\n",
    "from torch.nn import DataParallel\n",
    "# noinspection PyProtectedMember\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.bags2shoes import Bags2ShoesDataset, Bags2ShoesDataloader\n",
    "from modules.cycle_gan import CycleGAN\n",
    "from utils.dep_free import get_tqdm\n",
    "from utils.ifaces import FilesystemDataset\n",
    "from utils.metrics import GanEvaluator\n",
    "\n",
    "###################################\n",
    "###  Hyper-parameters settings  ###\n",
    "###################################\n",
    "# TODO: finish this notebook and train in Colab/Kaggle\n",
    "#   - training\n",
    "n_epochs = 300\n",
    "batch_size = 64\n",
    "train_test_splits = [90, 10]  # for a 90% training - 10% evaluation set split\n",
    "#   - evaluation\n",
    "metrics_n_samples = 1000 if not run_locally else 2\n",
    "metrics_batch_size = 32 if not run_locally else 1\n",
    "f1_k = 3 if not run_locally else 1\n",
    "#   - visualizations / checkpoints steps\n",
    "display_step = 200\n",
    "checkpoint_step = 600\n",
    "metrics_step = 1800  # evaluate model every 3 checkpoints\n",
    "#   - dataset\n",
    "target_shape = 64\n",
    "target_channels = 3\n",
    "#   - CycleGAN configuration\n",
    "ccgan_config_id = f'default'  # as proposed in the original paper\n",
    "\n",
    "###################################\n",
    "###   Dataset Initialization    ###\n",
    "###################################\n",
    "#   - image transforms:\n",
    "#     If target_shape is different from load one, resize & crop. If target_shape is different from load shape,\n",
    "#     convert to grayscale.\n",
    "#     Update: Now done automatically if you set target_channels, target_shape when instantiating the dataloader.\n",
    "gen_transforms = Bags2ShoesDataset.get_image_transforms(target_shape=target_shape, target_channels=target_channels)\n",
    "#   - the dataloader used to access the training dataset of cross-scale/pose image pairs at every epoch\n",
    "#     > len(dataloader) = <number of batches>\n",
    "#     > len(dataloader.dataset) = <number of total dataset items>\n",
    "dataloader = Bags2ShoesDataloader(dataset_fs_folder_or_root=datasets_groot, batch_size=batch_size,\n",
    "                                  image_transforms=gen_transforms, splits=train_test_splits,\n",
    "                                  pin_memory=not run_locally, log_level=log_level)\n",
    "dataset = dataloader.dataset  # save training dataset as `dataset`\n",
    "#   - ensure dataset is fetched locally and unzipped\n",
    "if isinstance(dataset, FilesystemDataset):\n",
    "    dataset.fetch_and_unzip(in_parallel=False, show_progress=True)\n",
    "elif hasattr(dataset, 'dataset') and isinstance(dataset.dataset, FilesystemDataset):\n",
    "    dataset.dataset.fetch_and_unzip(in_parallel=False, show_progress=True)\n",
    "else:\n",
    "    raise TypeError('dataset must implement utils.ifaces.FilesystemDataset in order to be auto-downloaded and unzipped')\n",
    "#   - apply rudimentary tests\n",
    "assert issubclass(dataloader.__class__, DataLoader)\n",
    "assert len(dataloader) == len(dataset) // batch_size + (1 if len(dataset) % batch_size else 0)\n",
    "_bags, _shoes = next(iter(dataloader))\n",
    "assert tuple(_bags.shape) == (batch_size, target_channels, target_shape, target_shape)\n",
    "assert tuple(_shoes.shape) == (batch_size, target_channels, target_shape, target_shape)\n",
    "\n",
    "###################################\n",
    "###    Models Initialization    ###\n",
    "###################################\n",
    "#   - initialize evaluator instance (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
    "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataset, target_index=1, device=exec_device,\n",
    "                         condition_indices=(0,), n_samples=metrics_n_samples, batch_size=metrics_batch_size,\n",
    "                         f1_k=f1_k)\n",
    "#   - initialize model\n",
    "chkpt_step = args.chkpt_step\n",
    "try:\n",
    "    if chkpt_step == 'latest':\n",
    "        ccgan_chkpt_step = chkpt_step\n",
    "    elif isinstance(chkpt_step, str) and chkpt_step.isdigit():\n",
    "        ccgan_chkpt_step = int(chkpt_step)\n",
    "    else:\n",
    "        ccgan_chkpt_step = None\n",
    "except NameError:\n",
    "    ccgan_chkpt_step = None\n",
    "ccgan = CycleGAN(model_fs_folder_or_root=models_groot, config_id=ccgan_config_id, dataset_len=len(dataset),\n",
    "                 chkpt_epoch=ccgan_chkpt_step, evaluator=evaluator, device=exec_device, log_level=log_level)\n",
    "ccgan.logger.debug(f'Using device: {str(exec_device)}')\n",
    "ccgan.logger.debug(f'Model initialized. Number of params = {ccgan.nparams_hr}')\n",
    "# FIX: Warmup counters before first batch\n",
    "if ccgan.step is None:\n",
    "    ccgan.gforward(batch_size=batch_size)\n",
    "    ccgan.logger.debug(f'Model warmed-up (internal counters).')\n",
    "#   - setup multi-GPU training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    ccgan.gen = DataParallel(ccgan.gen)\n",
    "    ccgan.info(f'Using {torch.cuda.device_count()} GPUs for CycleGAN Generator (via torch.nn.DataParallel)')\n",
    "#   - load dataloader state (from model checkpoint)\n",
    "if 'dataloader' in ccgan.other_state_dicts.keys():\n",
    "    dataloader.set_state(ccgan.other_state_dicts['dataloader'])\n",
    "    ccgan.logger.debug(f'Loaded dataloader state! Current pem_index={dataloader.get_state()[\"perm_index\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghwkg85n-NkX"
   },
   "source": [
    "### CycleGAN Main training loop\n",
    "\n",
    "Start/continue training CycleGAN until reaching the desired number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RwWCR4-O-NkX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[32mINFO    \u001B[0m | \u001B[32m[training loop] STARTING (epoch=0, step=1)\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[START OF EPOCH] {'step': 1, 'initial_step': 1, 'epoch': 0, '_counter': 64, 'epoch_inc': False}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296e917c36f14dd49879246c9c65b742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1952.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[END OF EPOCH] {'step': 1952, 'initial_step': 0, 'epoch': 0, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[START OF EPOCH] {'step': 1952, 'initial_step': 0, 'epoch': 0, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d689e4643c143f78ab81a2adc71e20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1952.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[SAMPLER] Reached end of epoch. Resetting state... (shuffle = True)\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[END OF EPOCH] {'step': 3904, 'initial_step': 0, 'epoch': 1, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[START OF EPOCH] {'step': 3904, 'initial_step': 0, 'epoch': 1, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa411e27bb747618ef82ea754bd415b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1952.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[SAMPLER] Reached end of epoch. Resetting state... (shuffle = True)\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[END OF EPOCH] {'step': 5856, 'initial_step': 0, 'epoch': 2, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[START OF EPOCH] {'step': 5856, 'initial_step': 0, 'epoch': 2, '_counter': 124891, 'epoch_inc': True}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d688d745d3f14b45a92ac04738aaeea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1952.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37m[SAMPLER] Reached end of epoch. Resetting state... (shuffle = True)\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32msrc/train_setup.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0mbags\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0mshoes\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mbags\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshoes\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mexec_tqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mccgan\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitial_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0;31m# Transfer image batches to GPU\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mbags\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbags\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexec_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/tqdm/notebook.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    233\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 234\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtqdm_notebook\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    235\u001B[0m                 \u001B[0;31m# return super(tqdm...) will not catch exception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    236\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1165\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1166\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1167\u001B[0m                 \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 435\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    473\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 475\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    476\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch-env/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/gans-thesis/src/datasets/bags2shoes.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    116\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_handbag_image\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_shoe_image\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_handbag_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_shoe_image\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "###################################\n",
    "###       Training Loop         ###\n",
    "###################################\n",
    "#   - get the correct tqdm instance\n",
    "exec_tqdm = get_tqdm()\n",
    "#   - start training loop from last checkpoint's epoch and step\n",
    "torch.cuda.empty_cache()\n",
    "gcapture_ready = True\n",
    "async_results = None\n",
    "ccgan.logger.info(f'[training loop] STARTING (epoch={ccgan.epoch}, step={ccgan.initial_step})')\n",
    "for epoch in range(ccgan.epoch, n_epochs):\n",
    "    # noinspection PyProtectedMember\n",
    "    d = {\n",
    "        'step': ccgan.step,\n",
    "        'initial_step': ccgan.initial_step,\n",
    "        'epoch': ccgan.epoch,\n",
    "        '_counter': ccgan._counter,\n",
    "        'epoch_inc': ccgan.epoch_inc,\n",
    "    }\n",
    "    # initial_step = ccgan.initial_step % len(dataloader)\n",
    "    ccgan.logger.debug('[START OF EPOCH] ' + str(d))\n",
    "\n",
    "    bags: Tensor\n",
    "    shoes: Tensor\n",
    "    for bags, shoes in exec_tqdm(dataloader, initial=ccgan.initial_step):\n",
    "        # Transfer image batches to GPU\n",
    "        bags = bags.to(exec_device)\n",
    "        shoes = shoes.to(exec_device)\n",
    "\n",
    "        # Perform a forward + backward pass + weight update on the Generator & Discriminator models\n",
    "        ccgan.gforward(bags.shape[0])\n",
    "\n",
    "    #     # Metrics & Checkpoint Code\n",
    "    #     if ccgan.step % checkpoint_step == 0:\n",
    "    #         # Check if another upload is pending\n",
    "    #         if not gcapture_ready and async_results:\n",
    "    #             # Wait for previous upload to finish\n",
    "    #             ccgan.logger.warning('Waiting for previous gcapture() to finish...')\n",
    "    #             [r.wait() for r in async_results]\n",
    "    #             ccgan.logger.warning('DONE! Starting new capture now.')\n",
    "    #         # Capture current model state, including metrics and visualizations\n",
    "    #         async_results = ccgan.gcapture(checkpoint=True, metrics=ccgan.step % metrics_step == 0, visualizations=True,\n",
    "    #                                        dataloader=dataloader, in_parallel=True, show_progress=True,\n",
    "    #                                        delete_after=False)\n",
    "    #     # Visualization code\n",
    "    #     elif ccgan.step % display_step == 0:\n",
    "    #         visualization_img = ccgan.visualize()\n",
    "    #         visualization_img.show() if not in_notebook() else display(visualization_img)\n",
    "    #\n",
    "    #     # Check if a pending checkpoint upload has finished\n",
    "    #     if async_results:\n",
    "    #         gcapture_ready = all([r.ready() for r in async_results])\n",
    "    #         if gcapture_ready:\n",
    "    #             ccgan.logger.info(f'gcapture() finished')\n",
    "    #             if ccgan.latest_checkpoint_had_metrics:\n",
    "    #                 ccgan.logger.info(str(ccgan.latest_metrics))\n",
    "    #             async_results = None\n",
    "    #\n",
    "    #     # If run locally one pass is enough\n",
    "    #     if run_locally and gcapture_ready:\n",
    "    #         break\n",
    "    #\n",
    "    # # If run locally one pass is enough\n",
    "    # if run_locally:\n",
    "    #     break\n",
    "\n",
    "    # noinspection PyProtectedMember\n",
    "    d = {\n",
    "        'step': ccgan.step,\n",
    "        'initial_step': ccgan.initial_step,\n",
    "        'epoch': ccgan.epoch,\n",
    "        '_counter': ccgan._counter,\n",
    "        'epoch_inc': ccgan.epoch_inc,\n",
    "    }\n",
    "    ccgan.logger.debug('[END OF EPOCH] ' + str(d))\n",
    "\n",
    "# Check if a pending checkpoint exists\n",
    "if async_results:\n",
    "    ([r.wait() for r in async_results])\n",
    "    ccgan.logger.info(f'last gcapture() finished')\n",
    "    if ccgan.latest_checkpoint_had_metrics:\n",
    "        ccgan.logger.info(str(ccgan.latest_metrics))\n",
    "    async_results = None\n",
    "\n",
    "# Training finished!\n",
    "ccgan.logger.info('[training loop] DONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IBJPgDI-NkY"
   },
   "source": [
    "# 3) Evaluate CycleGAN\n",
    "In this section we evaluate the generation performance of our trained network using the SOTA GAN evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwtVO9vH-NkY"
   },
   "source": [
    "## 3.1) Get the metrics evolution plots\n",
    "We plot how the metrics evolved during training. The GAN is **not** trained to minimize those metrics (they are\n",
    "calculated using `torch.no_grad()`) and thus this evolution merely depends on the network and showcases the correlation\n",
    "between the GAN evaluation metrics, and the losses (e.g. adversarial & reconstruction) used to optimize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpNgcsLr-NkY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Since the CycleGAN implements utils.ifaces.Visualizable, we can\n",
    "# directly call visualize_metrics() on the model instance.\n",
    "_ = ccgan.visualize_metrics(upload=True, preview=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsYiy5gP-NkY"
   },
   "source": [
    "## 3.2) Evaluate Generated Samples\n",
    "In order to evaluate generated samples and compare model with other GAN architectures trained on the same dataset.\n",
    "For this purpose we will re-calculate the evaluation metrics as stated above, but with a much bigger number of samples.\n",
    "In this way, the metrics will be more trustworthy and comparable with the corresponding metrics in the original paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqL_pDkc-NkZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a new evaluator instance\n",
    "# (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
    "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataloader.test_set, target_index=1,\n",
    "                         condition_indices=(0,), n_samples=10000, batch_size=metrics_batch_size,\n",
    "                         device=exec_device, f1_k=f1_k, ssim_c_img=target_channels)\n",
    "\n",
    "# Run the evaluator\n",
    "metrics_dict = evaluator.evaluate(gen=ccgan.gen, metric_name='all', show_progress=True)\n",
    "\n",
    "# Print results\n",
    "import json\n",
    "\n",
    "print(json.dumps(metrics_dict, indent=4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pxldtg_debug.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}