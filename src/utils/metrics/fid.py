import os
import sys
from typing import Optional, Union

import numpy as np
import torch
import torch.nn as nn
from IPython import get_ipython
from scipy import linalg
from torch import Tensor
# noinspection PyProtectedMember
from torch.utils.data import Dataset, DataLoader
from torchvision.models import inception_v3
from torchvision.transforms import transforms
from tqdm import tqdm

from dataset.deep_fashion import ICRBCrossPoseDataset
from modules.generators.pgpg import PGPGGenerator


class FID(nn.Module):
    """
    FID Class:
    This class is used to compute the Fréchet Inception Distance (FID) between two given image sets.
    """

    InceptionV3Transforms = transforms.Compose([
        # transforms.Resize(299),
        # transforms.CenterCrop(299),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    def __init__(self, chkpts_root: str = '/home/achariso/PycharmProjects/gans-thesis/.checkpoints',
                 device: str = 'cpu', n_samples: int = 512, batch_size: int = 8, crop_fc: bool = True):
        """
        FID class constructor.
        :param chkpts_root: absolute path to model checkpoints directory
        :param device: the device type on which to run the Inception model (defaults to 'cpu')
        :param n_samples: the total number of samples used to compute the metric (defaults to 512; the higher this
                          number gets, the more accurate the metric is)
        :param batch_size: the number of samples to precess at each loop
        :param crop_fc: set to True to crop FC layer from Inception v3 network
        """
        super(FID, self).__init__()
        self.inside_colab = 'google.colab' in sys.modules or \
                            'google.colab' in str(get_ipython()) or \
                            'COLAB_GPU' in os.environ
        if self.inside_colab:
            chkpt_path = '/content/drive/MyDrive/Model Checkpoints'
            device = 'cuda'
        else:
            chkpt_path = os.path.join(chkpts_root, 'inception_v3_google-1a9a5a14.pth')

        # Confirm checkpoint exists
        assert os.path.exists(chkpt_path)
        # Instantiate Inception v3 model
        self.inception = inception_v3(pretrained=False, init_weights=False)
        self.inception.load_state_dict(torch.load(chkpt_path))
        self.inception \
            .to(device) \
            .eval()
        # Cutoff FC layer from Inception model since we do not want classification, just the feature embedding
        if crop_fc:
            self.inception.fc = nn.Identity()

        # Save params in instance
        self.device = device
        self.n_samples = n_samples
        self.batch_size = batch_size

    @staticmethod
    def get_covariance(v: Tensor) -> Tensor:
        """
        Function to compute the covariance matrix of input vector.
        :param v: the input vector of shape (1, n_vars)
        :return: a torch.Tensor object of shape (n_vars, n_vars)
        """
        return torch.Tensor(np.cov(v.numpy(), rowvar=False))

    @staticmethod
    def matrix_sqrt(mat: Tensor) -> Tensor:
        """
        Function that takes in a matrix and returns the square root of that matrix.
        For an input matrix A, the output matrix B would be such that B @ B is the matrix A.
        :param mat: a matrix
        """
        sqrt_mat = mat.cpu().detach().numpy()
        sqrt_mat = linalg.sqrtm(sqrt_mat)
        return torch.Tensor(sqrt_mat.real, device=mat.device)

    @staticmethod
    def frechet_distance(x_mean: Tensor, y_mean: Tensor, x_cov: Tensor, y_cov: Tensor) -> Tensor:
        """
        Method for returning the Fréchet distance between multivariate Gaussians, parameterized by their means and
        covariance matrices.
        :param x_mean: the mean of the first Gaussian, (n_vars)
        :param y_mean: the mean of the second Gaussian, (n_vars)
        :param x_cov: the covariance matrix of the first Gaussian, (n_vars, n_vars)
        :param y_cov: the covariance matrix of the second Gaussian, (n_vars, n_vars)
        :return: a torch.Tensor object containing the Frechet distance of the two multivariate Gaussian distributions
        """
        return torch.norm(x_mean - y_mean) ** 2 + torch.trace(x_cov + y_cov - 2 * FID.matrix_sqrt(x_cov @ y_cov))

    def forward(self, dataset: Dataset, gen: nn.Module, target_index: Optional[int] = None,
                condition_indices: Optional[Union[int, tuple]] = None, z_dim: Optional[int] = None) -> float:
        """
        Compute the Fréchet Inception Distance between random $self.n_samples$ images from the given dataset and same
        number of images generated by the given generator network.
        :param dataset: a torch.utils.data.Dataset object to access real images
        :param gen: the Generator network
        :param target_index: index of target (real) output from the arguments that returns dataset::__getitem__() method
        :param condition_indices: indices of images that will be passed to the Generator in order to generate fake
                                  images (for image-to-image translation tasks). If set to None, the generator is fed
                                  with random noise.
        :param z_dim: if $condition_indices$ is None, then this is necessary to produce random noise to feed into the
                      DCGAN-like generator
        :return: a scalar value with the computed FID
        """
        gen.eval()
        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)

        cur_samples = 0
        real_embeddings_list = []
        fake_embeddings_list = []
        with torch.no_grad():
            for real_samples in tqdm(dataloader, total=self.n_samples // self.batch_size):
                if cur_samples >= self.n_samples:
                    break

                target_output = real_samples[target_index] if target_index is not None else real_samples
                cur_batch_size = len(target_output)
                gen_inputs = [real_samples[_i] for _i in condition_indices] if condition_indices is not None else \
                    torch.randn(cur_batch_size, z_dim)

                # Compute real embeddings
                target_output = target_output.to(self.device)
                if self.device.startswith('cuda'):
                    with torch.cuda.amp.autocast:
                        real_embeddings = self.inception(target_output)
                else:
                    real_embeddings = self.inception(target_output)
                real_embeddings_list.append(real_embeddings.detach().cpu())

                # Compute fake embeddings
                gen_inputs = [gen_input.to(self.device) for gen_input in gen_inputs] if condition_indices is not None \
                    else gen_inputs.to(self.device)
                if self.device.startswith('cuda'):
                    with torch.cuda.amp.autocast:
                        fake_output = gen(*gen_inputs)
                        if type(fake_output) == tuple or type(fake_output) == list:
                            fake_output = fake_output[-1]
                        fake_embeddings = self.inception(fake_output)
                else:
                    fake_output = gen(*gen_inputs)
                    if type(fake_output) == tuple or type(fake_output) == list:
                        fake_output = fake_output[-1]
                    fake_embeddings = self.inception(fake_output)
                fake_embeddings_list.append(fake_embeddings.detach().cpu())

                cur_samples += cur_batch_size

            # Compute sample mean and covariance of real embeddings
            real_embeddings = torch.cat(real_embeddings_list, dim=0)
            real_embeddings_mean = torch.mean(real_embeddings, dim=0)
            real_embeddings_cov = FID.get_covariance(real_embeddings)

            # Compute sample mean and covariance of fake embeddings
            fake_embeddings = torch.cat(fake_embeddings_list, dim=0)
            fake_embeddings_mean = torch.mean(fake_embeddings, dim=0)
            fake_embeddings_cov = FID.get_covariance(fake_embeddings)

        return FID.frechet_distance(real_embeddings_mean, fake_embeddings_mean,
                                    real_embeddings_cov, fake_embeddings_cov).item()


if __name__ == '__main__':
    _fid = FID(n_samples=2, batch_size=1)
    _dataset = ICRBCrossPoseDataset(image_transforms=FID.InceptionV3Transforms, pose=True)
    _gen = PGPGGenerator(c_in=6, c_out=3)
    fid = _fid(_dataset, _gen, target_index=1, condition_indices=(0, 2))
    print(fid)
